{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finetuning LLaMA 3 8b / Gemma 2b model using qlora (quantized Low Rank Adaptation) for parameter efficent finetuning.\n",
        "Using the Unloth pakage for speed up the finetuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# # Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-rrkw1bmi/unsloth_110e1b2e12544f889c22e08ec5385cf2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-rrkw1bmi/unsloth_110e1b2e12544f889c22e08ec5385cf2\n",
            "\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 27fa021a7bb959a53667dd4e7cdb9598c207aa0d\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: tyro in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.4)\n",
            "Requirement already satisfied: transformers>=4.38.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.41.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\n",
            "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
            "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.8)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
            "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
            "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.2)\n",
            "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
            "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.0)\n",
            "Requirement already satisfied: ninja in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.11.1.1)\n",
            "Requirement already satisfied: einops in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.8.0)\n",
            "Requirement already satisfied: flash-attn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.5.9.post1)\n",
            "Requirement already satisfied: xformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.27.dev792)\n",
            "Requirement already satisfied: trl in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.8.6)\n",
            "Requirement already satisfied: peft in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.11.1)\n",
            "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.30.1)\n",
            "Requirement already satisfied: bitsandbytes in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.43.1)\n",
            "Requirement already satisfied: triton in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.3.0)\n",
            "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: xformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.27.dev792)\n",
            "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from xformers) (1.26.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from xformers) (2.3.0)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (4.12.0)\n",
            "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.3.0->xformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->xformers) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch==2.3.0->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch==2.3.0->xformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass\n",
        "!pip install triton transformers\n",
        "!pip install -U datasets\n",
        "!pip install --pre -U xformers ##### this take some time\n",
        "\n",
        "\n",
        "# restart the kernel after running this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.\n",
        "* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "13af788235ff4578bba9edc148e6aeec",
            "572e623cf04d479580f1cb2e756720ac",
            "018ab020e6be4b8ba620869b58bb4828",
            "92a9d915186642f3834c494367a76610",
            "a76366420bf14bda92836b15007942c5",
            "ee0f25cb1363446d967a6bd49040dc26",
            "4fcf13908d344624839d5487451c994a",
            "e388e0f884e24a478f6830588adcf31b",
            "4b781af73fd549779c51d6741cf6428b",
            "41efbbbd389446f7b2c9e67f8c5bbfdb",
            "67edcd5d0f934979bfbb8e6e497b2736",
            "344e1370fbbe4bfe84eb4278a5966230",
            "db22add3a638403ca8e44356126dd27e",
            "1173a86f3daa491f87322ccc85ef8777",
            "c795fbe223a84de38f568e2737665aaf",
            "e1ac002080c9496abc65212dc72dd2cc",
            "4b4dc718ddd14ce7ac4c03cfa6a2d0d4",
            "e409618e3bc5485a966be9411ae15ff7",
            "fcab7a9778344ac8a18bfe4c7ce8f28c",
            "191163bcb4f547cc85d78ef0d0fca7da",
            "aebed33ac8454dc1962ca4fcdc1235f8",
            "455608c864264da3bc94ac3df41993cb",
            "6e65d8077d5c4b279bcd475b5ce83775",
            "5a02c351cb54430aa38b45c465f76923",
            "0d04d38279ac4f57a0b67539da427845",
            "41343c41d48642e69a5a01d4efbbf621",
            "6a7dedf385bd43c8a93aef7a1b612830",
            "645c740ed41e45829b76daf4f76e79e2",
            "c00b081adb164e2b899e76e7a006a3d7",
            "d074ce249fea43658da700ead7443790",
            "4c9853f12a82474a9726db291f541d7a",
            "398ae62a5de6496385df9c20515e6a6d",
            "1c70c42a97234e6391bdcd15fee4558d",
            "f3b9aee644ec406a9ab4fda91ed57f15",
            "77814e7ac2de4e75a547fd809feba4c8",
            "43cb7d5b4c0346cdbf24a5df47fe3fa7",
            "abd0f68b5da4482eae24b045b594885c",
            "24a1b46a40ad4023a836e92727b3f013",
            "ab63a7d6d98b44c4a0c8645e3233a333",
            "77bb0d06fac64e8d9872833c81484267",
            "6b99e709dc3a4336b8bb32cddc24def0",
            "a961c37f09154c0ca65bb9c8564a559f",
            "ce2ae5441b974c0093fe159715b8082c",
            "3d8a32787d1b4cd49921d042f2a5e9d8",
            "d2ba5d838025441dac0e8a3290674336",
            "acec15bf8cfa4bad87adb70da0837f5c",
            "24b7c17d00094a5a96a2e5e79a3fdee6",
            "1d8817ad252d4afba3d53abd46f6d236",
            "8235d8d50b744ba9bbeb5dc4cb1593ba",
            "0614e9ed8b6e44449fe65ed80a5c8b6f",
            "be61491e3636434595d31a17742b93b4",
            "36aacaf786a4497b81fb32dc1e7de178",
            "74b80031714f49bfa4a764ac139d9d53",
            "4b4aec7690784ed8a52be5f4ea1ffee3",
            "03c028a3344e4269883918843c329c27",
            "9535c432363049758b98437902fd2899",
            "64a3f325e1a5488c9adffffcddfc4d63",
            "884c6c053ce248c6a90aafb4eff9c2a3",
            "65a7faa239474335a6b1fc474acf7672",
            "c3df08065045437fb270f224918a7484",
            "ce862857332d469a9bb111c8621928a9",
            "352fef96741d4a06b076f1f123e78550",
            "9ec75a7ee7f34f689105e2fe84b9cc02",
            "0c5d57844c3045efa006349c1c7b44f1",
            "ba2ebbcc709444f6b38ff82bf2319886",
            "46bca0d0fafa45d1969f64014524fee9"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "7a4f8b11-5081-4457-feed-e099281f93e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
            "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.27.dev792. FA = True.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 8048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SDD_iCOhv0Jz"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# df_evaluated = pd.read_pickle(\"/content/0122_10000_evaluated.pkl\")\n",
        "df_evaluated = pd.read_pickle(os.path.join(os.getcwd(), \"0122_10000_evaluated.pkl\"))\n",
        "# df_news = pd.read_pickle(\"/content/mixtral_integrated_df.pkl\")\n",
        "# df_news = pd.read_pickle(\"/content/3_new_days_mixtral_integrated_df.pkl\")\n",
        "df_news = pd.read_pickle(os.path.join(os.getcwd(), \"3_new_days_mixtral_integrated_df.pkl\"))\n",
        "df_news = df_news[df_news['answer'] != 'Error: LLM call failed']\n",
        "df_evaluated = df_evaluated[df_evaluated[\"accuracy\"] > 4.5]\n",
        "# df_evaluated.head(3)\n",
        "# df_news.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jC5XQw3zbls",
        "outputId": "0c32ca61-37c5-46a2-d34e-c6d20563a051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 5)\n",
            "(1000, 5)\n",
            "(1000, 5)\n",
            "(1000, 5)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define the number of instances to select per language\n",
        "split_language = 1000\n",
        "\n",
        "# Create a dictionary to store language-specific DataFrames\n",
        "language_dataframes = {\n",
        "    lang: df_evaluated[df_evaluated[\"language\"] == lang].sample(split_language, random_state=42)\n",
        "    for lang in df_evaluated[\"language\"].unique()\n",
        "}\n",
        "\n",
        "# Access the DataFrames for each language using the dictionary\n",
        "df_finetuning_en = language_dataframes[\"en\"]  # Access English DataFrame\n",
        "df_finetuning_it = language_dataframes[\"it\"]  # Access Italian DataFrame (if it exists)\n",
        "df_finetuning_es = language_dataframes[\"es\"]  # Access Spanish DataFrame (if it exists)\n",
        "df_finetuning_fr = language_dataframes[\"fr\"]  # Access French DataFrame (if it exists)\n",
        "\n",
        "# Print DataFrame shapes\n",
        "print(df_finetuning_en.shape)\n",
        "print(df_finetuning_it.shape)\n",
        "print(df_finetuning_es.shape)\n",
        "print(df_finetuning_fr.shape)\n",
        "\n",
        "df_finetuning = pd.concat([df_finetuning_en, df_finetuning_it, df_finetuning_es, df_finetuning_fr], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-em6KnCX24BW"
      },
      "outputs": [],
      "source": [
        "df_news['language'] = 'it'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "Xhg4enyx2YbW",
        "outputId": "cbb5336d-fa16-4f25-eabd-336171e26621"
      },
      "outputs": [],
      "source": [
        "# Merge the selected dataframes based on 'question' and 'answer'\n",
        "merged_df = pd.merge(df_finetuning, df_news, on=['question', 'answer', 'language'], how='outer')\n",
        "# merged_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FXDnRUx3yPHM"
      },
      "outputs": [],
      "source": [
        "# rename the columns\n",
        "merged_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
        "df_finetuning = merged_df.rename(columns = {\"question\": \"instruction\", \"answer\": \"output\" , \"language\": \"input\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "957538735f464d3595cd5d8efccbd2c7",
            "9204c1f083154b48884d7edd5acdc0c6",
            "0053461480574f4ca01e5fe759286134",
            "08d064adf184414aab0a7ad39ac7ce3d",
            "5b2e1c1885a84fc29e977ac0364664a4",
            "adcc1da4bf404636bfd5edded10b721f",
            "9e07206c8dcf4e198b49f61824a3b122",
            "875fa70db29f44359394192f265e0cf6",
            "3f7d79b14e30402aa410596c1640e92f",
            "fad7f33594534d109f1d160ee14f67a4",
            "7fa3608e6c3442ec8d34179c5e8770da"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "60c1ffba-1936-4092-f593-feac386a57ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ade5c35d3d524fcfa432c70dc04eb567",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4171 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context (if present). Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "# dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "# dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(df_finetuning)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test the model\n",
        "\n",
        "def prompt_inference(prmpt):\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        prmpt\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
        "    return tokenizer.batch_decode(outputs)[0].split(\"### Response:\")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" \\n```python\\n# Cos'la serie di fibonacci, restituisci per 10 elementi commentandoli testualmente\\ndef fibonacci(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\nfor i in range(10):\\n    print(fibonacci(i))\\n```<|end_of_text|>\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Cos'la serie di fibonacci, restituisci per 10 elementi commentandoli testualmente\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "print(\"result\")\n",
        "prompt_inference(prmpt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test_it_before_training = df_finetuning[df_finetuning['input']=='it']['instruction'].iloc[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[**]Prompt 1\n",
            "Write a Lua script that generates random maze patterns using Prim's algorithm.\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```lua\n",
            "function generate_maze(n, m)\n",
            "    local maze = {}\n",
            "\n",
            "    for i = 1, n do\n",
            "        maze[i] = {}\n",
            "        for j = 1, m do\n",
            "            maze[i][j] = {}\n",
            "        end\n",
            "    end\n",
            "\n",
            "    -- fill in with random values\n",
            "\n",
            "    return maze\n",
            "end\n",
            "\n",
            "function prim(maze, start)\n",
            "    local frontier = {}\n",
            "    local discovered = {}\n",
            "    local visited = {}\n",
            "\n",
            "    table.insert(frontier, start)\n",
            "\n",
            "    while #frontier > 0 do\n",
            "        local current = table.remove(frontier)\n",
            "\n",
            "        discovered[current] = true\n",
            "\n",
            "        for neighbor, value in pairs(maze[current]) do\n",
            "            if not discovered[neighbor] then\n",
            "                table.insert(frontier, neighbor)\n",
            "            end\n",
            "        end\n",
            "    end\n",
            "\n",
            "    return visited\n",
            "end\n",
            "\n",
            "local maze = generate_maze(5, 5)\n",
            "\n",
            "local start = 1\n",
            "local end = 5\n",
            "\n",
            "prim(maze, start\n",
            "\n",
            "\n",
            "[**]Prompt 6\n",
            "Write a PHP script that takes user input from a web form and stores it in a MySQL database.\n",
            "[*]result:\n",
            "<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 9\n",
            "Vedi una barca piena di gente. Non è affondata, ma se guardi di nuovo non vedi una sola persona sulla barca. Perché?\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It was the Titanic.<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 14\n",
            "Considerati più titoli e contenuti degli articoli, riassumili e integrali in un unico testo.\n",
            "        Input: Title: Mitsotakis: 'L'immigrazione può risolvere carenza di manodopera' \n",
            " Content: ATENE - \"Non temiamo il termine integrazione; siamo una società aperta che ha dimostrato, in passato, la volontà di accogliere coloro che cercano di integrarsi nella società greca. Rendere la loro vita permanente qui nel tempo è un passo naturale\". Lo ha dichiarato il premier greco, Kyriakos Mitsotakis, durante una conferenza ad Atene dal nome 'Soluzioni europee alla sfida comune della migrazione', alla presenza del vicepresidente della Commissione europea, Margaritis Schinas, e della commissaria europea per gli Affari interni, Ylva Johansson. Lo riporta il sito di Kathimerini. \"Nel 2023 abbiamo gestito i flussi migratori in modo più efficace rispetto a molti dei nostri partner\", ha ricordato Mitsotakis, sottolineando che \"la migrazione non è necessariamente un argomento divisivo\" e che alternative legali per una migrazione controllata possono colmare le lacune di manodopera nel mercato del lavoro. \"Possiamo trovare un equilibrio, la Grecia può fare da apripista\", ha spiegato il premier greco. Nel mese scorso, il parlamento greco ha votato una legge che concede un permesso di residenza e di lavoro di tre anni agli immigrati irregolari che rispettino determinati parametri, con l'obiettivo di gestire la carenza di manodopera in alcuni settori economici. Secondo Kathimerini, si stima che circa 30.000 persone, provenienti principalmente da Albania, Georgia, Pakistan e Filippine, richiederanno questo nuovo tipo di permesso. \n",
            "\n",
            "\n",
            "Riproduzione riservata © Copyright ANSA\n",
            "Title: Matteo Salvini anticipa Giorgia Meloni, il vicepremier si smarca dalle Europee: «Continuo a fare il ministro» \n",
            " Content: Matteo Salvini non ha intenzione di candidarsi alle prossime elezioni europee. Ospite di Nicola Porro a Quarta Repubblica su Rete4, il vicepremier ha chiarito che non ci sarà il suo nome nelle liste del Carroccio: «Non mi candido alle elezioni europee – ha detto Salvini – Continuerà a fare il ministro». Salvini ha anticipato la discussione che Giorgia Meloni aveva ipotizzato lo scorso 4 gennaio, quando durante la conferenza stampa annuale non aveva del tutto escluso la possibilità di candidarsi per Fratelli d’Italia alle Europee: «Lo decideremo insieme nel centrodestra – aveva detto la premier – devo assicurarmi che non tolga tempo al mio lavoro da presidente del Consiglio». Quel confronto interno nella maggioranza a questo punto non sarà necessario, sempre se Salvini non torni sui suoi passi. Il leader della Lega spera però di riuscire a convincere a correre per la Lega il generale Roberto Vannacci, che ha confermato essere ancora tra i suoi obiettivi per le europee. Poco prima da Udine, però, era stato lo stesso generale a mettere in chiaro che l’ultima parola spetta ancora a lui: «Non ho mai detto di essermi candidato, anzi a tutti quelli che mi hanno chiesto se mi sarei candidato ho sempre risposto che continuo a fare il soldato, e se cambierò idea sarò io che lo comunicherò agli organi di informazione, non c’è bisogno che loro precedano questa mia eventuale dichiarazione».\n",
            "\n",
            "Leggi anche:\n",
            "\n",
            "Cristina Seymandi candidata alle Europee con la Lega? Così Salvini medita il colpo a sorpresa in Piemonte\n",
            "Le Europee secondo Tajani: «Rischioso candidare i leader. Le Pen? Impossibile fare accordi con chi è anti-Ue e anti-Nato»\n",
            "Nonostante fughe e tradimenti, la Lega dimezzerà i suoi a Strasburgo. Meloni raddoppierà, problemi per Pd e Forza Italia, non per il M5s\n",
            "Title: Governo Madrid a catalani: 'La politica migratoria la detta l'Europa' \n",
            " Content: MADRID - Non si placa la polemica politica suscitata dall'intesa del governo con il partito indipendentista JuntsXCat per la cessione alla Catalogna delle competenze \"integrali\" in materia di immigrazione, come contropartita al via libera a tre decreti chiave. Accordo che ha provocato frizioni del partito dell'ex presidente catalano Carles Puigdemont con i soci repubblicani di Erc al governo della Generalitat, tenuti all'oscuro. Il ministro di Presidenza, Giustizia e Rapporti con il parlamento, Felix Bolanos, ha avvertito oggi che \"la politica migratoria è una politica europea\" e che \"gli orientamenti vengono dall'Europa\", in dichiarazioni ai cronisti dopo un incontro con il presidente dell'alto tribunale dell'Audiencia nacional. Come già ieri, Bolanos non ha fornito dettagli sulla cessione di competenze concordata dall'esecutivo del Psoe con Junts e ha rimandato alla futura legge organica che dovrà articolarla \"nel solco istituzionale\". Ma ha fatto anche riferimento al patto europeo di migrazione e asilo, approvato a dicembre nel semestre di presidenza spagnola della Ue, \"dove si stabilisce con chiarezza che la politica migratoria è una politica europea e pertanto gli orientamenti vengono dall'Europa\". Ha aggiunto che \"è importante\" che chi eserciti le competenze in materia \"lo faccia \"negli orientamenti che indica il patto\" e la Commissione europea. Intanto, in Catalogna, la numero due del governo della Generalitat, Laura Villagrà, dopo un colloquio telefonico con Bolanos, ha confermato che \"per ora non c'è nulla di negoziato\", in attesa della definizione della legge organica. Si tratta di \"una dichiarazione di intenti\" anche per la portavoce di Erc, Raquel Sans, che in dichiarazioni a La\" e Radio4 ha segnalato che il suo partito è d'accordo con la cessione di competenze alla Catalogna, ma discute il modo in cui si è prodotta. \n",
            "\n",
            "\n",
            "Riproduzione riservata © Copyright ANSA\n",
            "\n",
            "        Output Summary in italian: \n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La politica migratoria la detta l'Europa. \n",
            "Non si placa la polemica politica suscitata dall'intesa del governo con il partito indipendentista JuntsXCat per la cessione alla Catalogna delle competenze \"integrali\" in materia di immigrazione, come contropartita al via libera a tre decreti chiave. Accordo che ha provocato frizioni del partito dell'ex presidente catalano Carles Puigdemont con i soci repubblicani di Erc al governo della Generalitat, tenuti all'oscuro. Il ministro di Presidenza, Giustizia e Rapporti con il parlamento, Felix Bolanos, ha avvertito oggi che \"la politica migratoria è una politica europea\" e che \"gli orientamenti vengono dall'Europa\", in dichiarazioni ai cronisti dopo un incontro con il presidente dell'alto\n",
            "\n",
            "\n",
            "[**]Prompt 18\n",
            "Un treno lascia New York City a 60 mph mentre un altro treno lascia Los Angeles viaggiando a 80 mph. Quante banane ci sono su ogni treno?\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 120 bananas on each train.\n",
            "\n",
            "### Explanation:\n",
            "There are 120 bananas on each train. The first train is traveling at 60 miles per hour. The second train is traveling at 80 miles per hour. To find the number of bananas on each train, we must multiply the number of miles each train travels by the number of bananas on each train. The first train travels 60 miles, so there are 60 bananas on that train. The second train travels 80 miles, so there are 80 bananas on that train. To find the total number of bananas on each train, we must add the number of bananas on each train. The first train has 60 bananas, and the second train has 80 bananas. Therefore, there are 120 bananas on each train.\n",
            "\n",
            "### Comment:\n",
            "The response is correct, but it does not explain how the number of bananas on each train was calculated. The response should explain how the number of bananas on each train was calculated.\n",
            "\n",
            "###\n",
            "\n",
            "\n",
            "[**]Prompt 26\n",
            "Un parco a tema sta progettando di costruire nuove montagne russe con un costo di costruzione stimato di 10 milioni di dollari. Si prevede di attirare altri 200.000 visitatori all'anno grazie alla nuova attrazione. Il prezzo medio del biglietto è di $ 50 e si prevede che le spese operative del parco aumenteranno del 15% dopo la costruzione delle montagne russe. Calcola il periodo di recupero dell'investimento e fornisci una motivazione dettagliata.\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it\n",
            "<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 29\n",
            "Parte dell'ala di un uccello, questo palindromo aiuta con la stabilità del volo.\n",
            "[*]result:\n",
            "<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 33\n",
            "Data la domanda: Kendall ha chiesto a Tracy se voleva andare a vedere Lady Gaga; decise che non avrebbe accettato un no come risposta. Dato il contesto: cosa ha fatto Kendall? Possibili risposte: ha venduto i biglietti per Lady Gaga, ha deciso che avrebbe accettato solo una risposta sì, è andata a vedere Lady Gaga da sola\n",
            "La risposta è:\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ha venduto i biglietti per Lady Gaga\n",
            "<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 35\n",
            "Domanda: scegli l'opzione in linea con il buon senso per rispondere alla domanda. Domanda: John gioca a scacchi con il suo compagno di stanza. Fanno una mossa ciascuno tra una lezione e l'altra. Dove è molto probabilmente sistemato il suo set degli scacchi? Opzioni: A. Canada B. armadio C. dormitorio D. salotto E. cassetto\n",
            "Risposta:\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The set of chess is in the room of the dormitory.\n",
            "\n",
            "<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 36\n",
            "Informazioni: - Il tenente generale Ali Muhammad Jan Aurakzai (lingua urdu:), è un ufficiale generale in pensione di grado a tre stelle dell'esercito pakistano che ha servito come comandante del corpo dell'XI corpo e comandante principale del comando occidentale. In qualità di comandante, ha comandato tutte le risorse militari di combattimento e ha supervisionato il dispiegamento pacifico dell'XI Corpo nelle aree settentrionali e nelle aree tribali ad amministrazione federale (FATA). Aurakzai era il principale generale dell'esercito che guidò le forze combattenti del Pakistan in risposta all'invasione americana dell'Afghanistan in seguito agli attacchi terroristici negli Stati Uniti. Dopo essersi ritirato dall'esercito, è stato nominato governatore del Khyber-Pakhtunkhwa del Pakistan, dal maggio 2006 fino alle sue dimissioni nel gennaio 2008. - Una repubblica islamica è il nome dato a diversi stati in paesi governati da leggi islamiche, tra cui le repubbliche islamiche del Pakistan, Iran, Afghanistan e Mauritania. Il Pakistan adottò per la prima volta il titolo con la costituzione del 1956. La Mauritania lo adottò il 28 novembre 1958. L'Iran lo adottò dopo la rivoluzione iraniana del 1979 che rovesciò la dinastia Pahlavi. L’Afghanistan lo adottò nel 1992 (nel 1996-2001 i Talebani governavano come un emirato islamico (monarchia)) dopo che Jamiat-e Islami conquistò la capitale Kabul dai comunisti. Nonostante il nome simile, i paesi differiscono notevolmente nei loro governi e nelle loro leggi. - La Cina, ufficialmente Repubblica popolare cinese (RPC), è uno stato sovrano unitario dell'Asia orientale. Con una popolazione di oltre 1,381 miliardi, è il paese più popoloso del mondo. Lo stato è governato dal Partito Comunista Cinese e la sua capitale è Pechino. Esercita giurisdizione su 22 province, cinque regioni autonome, quattro municipalità controllate direttamente (Pechino, Tianjin, Shanghai e Chongqing) e due regioni amministrative speciali per lo più autonome (Hong Kong e Macao) e rivendica la sovranità su Taiwan. Le principali aree urbane del paese includono Shanghai, Guangzhou, Pechino, Chongqing, Shenzhen, Tianjin e Hong Kong. La Cina è una grande potenza e un’importante potenza regionale in Asia, ed è stata caratterizzata come una potenziale superpotenza. - L'Iran (, anche , ; ' ), noto anche come Persia, ufficialmente Repubblica islamica dell'Iran (' ), è uno stato sovrano dell'Asia occidentale. Confina a nord-ovest con l'Armenia, la Repubblica \"de facto\" del Nagorno-Karabakh, e l'Azerbaigian; a nord dal Mar Caspio; a nord-est dal Turkmenistan; a est dall'Afghanistan e dal Pakistan; a sud dal Golfo Persico e dal Golfo di Oman; e ad ovest dalla Turchia e dall'Iraq. Composto da una superficie di , è il secondo paese più grande del Medio Oriente e il 18esimo più grande del mondo. Con 82,8 milioni di abitanti, l'Iran è il 17° paese più popoloso del mondo. È l'unico paese con una costa sia del Mar Caspio che dell'Oceano Indiano. La posizione centrale del paese in Eurasia e nell'Asia occidentale, e la sua vicinanza allo Stretto di Hormuz, lo rendono di grande importanza geostrategica. Teheran è la capitale e la città più grande del paese, nonché il suo principale centro economico. - L'esercito pakistano (\"Pak Fauj\" (IPA: pk f~d); nome in codice: PA) è il ramo di servizio terrestre delle forze armate pakistane. È nato dopo l'indipendenza del Pakistan nel 1947. Secondo l'Istituto internazionale per gli studi strategici (IISS) aveva una forza attiva di circa 950.000 membri attivi nel 2017. In Pakistan, ci sono 1623 anni di età per il servizio militare volontario ; i soldati non possono essere schierati in combattimento fino all'età di 18 anni. L'esercito pakistano ha iniziato a reclutare donne come ufficiali incaricati. L'aeronautica militare e la marina pakistana hanno arruolato le loro prime donne pilota e marinaio nel 2012, vedere i dettagli in Donne nelle forze armate pakistane. - L'Istituto Internazionale per gli Studi Strategici (IISS) è un istituto di ricerca (o think tank) britannico nel campo degli affari internazionali. Dal 1997 la sua sede è Arundel House, a Londra, Inghilterra. Il Global Go To Think Tank Index 2013 ha classificato l'IISS come il nono miglior think tank a livello mondiale. - Il Pakistan (o ), ufficialmente Repubblica islamica del Pakistan, è una repubblica parlamentare federale dell'Asia meridionale al crocevia dell'Asia centrale e dell'Asia occidentale. È il sesto paese più popoloso con una popolazione che supera i 200 milioni di persone. È il 36esimo paese più grande del mondo in termini di superficie con una superficie di . Il Pakistan ha una linea costiera lungo il Mar Arabico e il Golfo di Oman a sud e confina rispettivamente con l'India a est, l'Afghanistan a ovest, l'Iran a sud-ovest e la Cina nell'estremo nord-est. È separato dal Tagikistan dallo stretto corridoio Wakhan dell'Afghanistan a nord e condivide anche un confine marittimo con l'Oman. - L'Afghanistan (Pashto/Dari: , \"Afnistn\"), ufficialmente Repubblica islamica dell'Afghanistan, è un paese senza sbocco sul mare situato tra l'Asia meridionale e l'Asia centrale. Ha una popolazione di circa 32 milioni di abitanti, il che lo rende il 42esimo paese più popoloso del mondo. Confina con il Pakistan a sud e ad est; l'Iran a ovest; Turkmenistan, Uzbekistan e Tagikistan a nord; e la Cina nell'estremo nord-est. Il suo territorio copre l', rendendolo il 41° paese più grande del mondo. - L'India, ufficialmente Repubblica dell'India (\"Bhrat Gaarjya\"), è un paese dell'Asia meridionale. È il settimo paese più grande per area, il secondo paese più popoloso (con oltre 1,2 miliardi di persone) e la democrazia più popolosa del mondo. È delimitato dall'Oceano Indiano a sud, dal Mar Arabico a sud-ovest e dal Golfo del Bengala a sud-est. Condivide i confini terrestri con il Pakistan a ovest; Cina, Nepal e Bhutan a nord-est; e Myanmar (Birmania) e Bangladesh a est. Nell'Oceano Indiano, l'India si trova nelle vicinanze dello Sri Lanka e delle Maldive. Le isole Andamane e Nicobare dell'India condividono un confine marittimo con Thailandia e Indonesia. La sua capitale è Nuova Delhi; altre metropoli includono Mumbai, Calcutta, Chennai, Bangalore, Hyderabad e Ahmedabad. - L'Islam (' ;) è una religione articolata dal Corano, un testo considerato dai suoi aderenti la parola letterale di Dio ('), e, per la stragrande maggioranza degli aderenti, gli insegnamenti e l'esempio normativo (chiamati \"Sunnah \", composto da resoconti chiamati \"hadith\") di Maometto (5708 giugno 632 d.C.). È la seconda religione più grande del mondo e la religione principale in più rapida crescita nel mondo, con oltre 1,7 miliardi di seguaci o il 23% della popolazione mondiale, noti come musulmani. L'Islam è una religione monoteista abramitica che sostiene che Dio è uno e incomparabile e che lo scopo dell'esistenza è adorare Dio. I musulmani considerano Maometto l'ultimo profeta di Dio. - Il Tagikistan (, o ), ufficialmente Repubblica del Tagikistan (\"Çumhuriji Toçikiston\"), è un paese montuoso e senza sbocco sul mare dell'Asia centrale con circa 8 milioni di abitanti nel 2013 e un'area di . Confina a sud con l'Afghanistan, a ovest con l'Uzbekistan, a nord con il Kirghizistan e a est con la Cina. Il Pakistan si trova a sud, separato dallo stretto corridoio Wakhan. Le terre d'origine tradizionali del popolo tagico includevano l'attuale Tagikistan, Afghanistan e Uzbekistan. Considerati i paragrafi precedenti, decidere quale entità ha la relazione \"data di nascita\" con \"1947\".\n",
            "[*]result:\n",
            "The Pakistan\n",
            "\n",
            "### Explanation:\n",
            "The Pakistan\n",
            "\n",
            "### Comments:\n",
            "The Pakistan\n",
            "\n",
            "###\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for idx, instruction in df_test_it_before_training.items():\n",
        "  prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "  ### Instruction:\n",
        "  {instruction}\n",
        "\n",
        "\n",
        "  ### Response:\"\"\"\n",
        "\n",
        "  print(f\"[**]Prompt {idx+1}\")\n",
        "  print(instruction)\n",
        "  #   print(prompt)\n",
        "  print(\"[*]result:\")\n",
        "  inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            instruction, # instruction\n",
        "            \"it\", # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 200, use_cache = True)\n",
        "  outputs = tokenizer.batch_decode(outputs)\n",
        "  result =outputs[0].split(\"Response:\")[-1].strip()\n",
        "\n",
        "  \n",
        "  print(result)\n",
        "  print(\"\\n\")  # Add a newline for better readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "f2ea5d199de44f88abc51eb3253962a1",
            "84f0c2b0468f4aefa5685232ce1dc608",
            "f5ca284460c1428b876ae6c322bb829f",
            "1c80de3ff70b49d0bd107e27f02f6d6e",
            "f764369eae084670a25685278fab6814",
            "3b45a982d1984594aa2cfbc9981cd9d3",
            "88bf54c07ccf4b13b9f3a802723492c3",
            "2ddd9d8036de401aa95bdae1c5ce454c",
            "ed343a2ecb4143328653d5ffb440badb",
            "78e75d8bfeab4e3dbaa85ea5a6f25aea",
            "a3113795374d488eae532f0e5d5849d5"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "e96f35a6-f89e-45bb-e0d7-ffc354bb4b40"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "372d1bc5b1a5440f80eb0f5e19da2259",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/4171 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 50,\n",
        "        # max_steps = 60,\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 2e-4,\n",
        "        # fp32 = True,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "be76e42d-d54f-4810-e4e8-ef210b6f5652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA L4. Max memory = 22.168 GB.\n",
            "7.0 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install the leatest version of torch and xFormers \n",
        "# ! pip install torch xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "29968a8f-b4ac-4289-f775-7e125f280474"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 4,171 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 4 | Total steps = 1,042\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1042' max='1042' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1042/1042 1:08:56, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.478900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.211500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.101900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.954900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.140700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.052300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.942000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.156900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.162900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.122900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.069600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.068800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.076300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.037200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.040100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.146500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.080200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.042800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.045000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.052400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.026800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.101600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.933000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.076500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.951000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.067300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.072300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.951600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.919400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.080300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.041300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.960100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.988200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.093600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.877300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.083700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.931400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.994000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.977800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.972100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.003700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.061100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.137900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.984500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.052500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.986400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.943200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.922600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.871300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.851300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.959000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.970800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.927900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.886500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.883100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.960500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>1.050100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.977700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.950400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.894100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.931500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.963200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.921800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.982800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>1.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.952800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>1.047400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.034300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.872400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.088900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.912000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.060800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.946200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.925500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.867000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.815500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.804700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.937400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.994800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.850900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.877700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.900300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.887700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.914600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.841100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.938700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.960400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.829600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>1.023500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.910100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>1.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.991300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.797900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.902200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.895100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.952000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.907900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.931100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.848600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.918600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.931800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.883300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4140.891 seconds used for training.\n",
            "69.01 minutes used for training.\n",
            "Peak reserved memory = 12.959 GB.\n",
            "Peak reserved memory for training = 5.959 GB.\n",
            "Peak reserved memory % of max memory = 58.458 %.\n",
            "Peak reserved memory for training % of max memory = 26.881 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context (if present). Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\nThe Fibonacci sequence continues as: 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267444597, 433494437, 701408733, 1134903170, 1836311903, 29712151<|end_of_text|>']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 250, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[**]Prompt 1\n",
            "Write a Lua script that generates random maze patterns using Prim's algorithm.\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```lua\n",
            "-- Prim's Algorithm for generating random maze patterns in Lua\n",
            "\n",
            "local function generate_maze(width, height)\n",
            "    local maze = {}\n",
            "    local visited = {}\n",
            "\n",
            "    for y = 1, height do\n",
            "        maze[y] = {}\n",
            "        for x = 1, width do\n",
            "            maze[y][x] = false\n",
            "            visited[y][x] = false\n",
            "        end\n",
            "    end\n",
            "\n",
            "    local start_x, start_y = math.random(width), math.random(height)\n",
            "    visited[start_y][start_x] = true\n",
            "    maze[start_y][start_x] = true\n",
            "\n",
            "    local queue = {{start_x, start_y}}\n",
            "\n",
            "    while #queue > 0 do\n",
            "        local current = table.remove(queue)\n",
            "        local x, y = current[1], current[2]\n",
            "\n",
            "        for _, neighbor in ipairs({{x - 1, y}, {x + 1, y}, {x, y - 1}, {x, y + 1}}) do\n",
            "            local nx, ny = neighbor[1], neighbor[2]\n",
            "\n",
            "            if nx >= 1 and nx <= width and ny >= 1 and ny <= height and not visited[ny][nx] then\n",
            "                visited[ny][nx] = true\n",
            "                maze[ny][nx] = true\n",
            "\n",
            "                table.insert(queue, {nx, ny})\n",
            "            end\n",
            "        end\n",
            "    end\n",
            "\n",
            "    return maze\n",
            "end\n",
            "\n",
            "local function print_maze(maze)\n",
            "    for y = 1, #maze do\n",
            "        for x = 1, #maze[y] do\n",
            "            print(maze[y][x] and \" \" or \"#\")\n",
            "        end\n",
            "    end\n",
            "end\n",
            "\n",
            "local function main()\n",
            "    local width, height = 15, 15\n",
            "    local maze = generate_maze(width, height)\n",
            "    print_maze(maze)\n",
            "end\n",
            "\n",
            "main()\n",
            "```\n",
            "\n",
            "This Lua script generates a random maze using Prim's algorithm. The generated maze is printed to the console.<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 6\n",
            "Write a PHP script that takes user input from a web form and stores it in a MySQL database.\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To create a PHP script that takes user input from a web form and stores it in a MySQL database, you need to have the following things set up:\n",
            "\n",
            "1. A web server with PHP support.\n",
            "2. A MySQL database with a table named \"users\" containing columns for ID, name, email, and password.\n",
            "\n",
            "Here's a simple example of how you can do this:\n",
            "\n",
            "```php\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "    <title>PHP Form to MySQL</title>\n",
            "</head>\n",
            "<body>\n",
            "\n",
            "<form action=\"store.php\" method=\"post\">\n",
            "    Name: <input type=\"text\" name=\"name\"><br>\n",
            "    Email: <input type=\"email\" name=\"email\"><br>\n",
            "    Password: <input type=\"password\" name=\"password\"><br>\n",
            "    <input type=\"submit\" value=\"Submit\">\n",
            "</form>\n",
            "\n",
            "</body>\n",
            "</html>\n",
            "```\n",
            "\n",
            "This is a simple HTML form that accepts user input for name, email, and password. When the user submits the form, it sends the data to a PHP script called \"store.php\".\n",
            "\n",
            "Now let's create the \"store.php\" file:\n",
            "\n",
            "```php\n",
            "<?php\n",
            "// Database credentials\n",
            "$servername = \"localhost\";\n",
            "$username = \"username\";\n",
            "$password = \"password\";\n",
            "$dbname = \"myDB\";\n",
            "\n",
            "// Create connection\n",
            "$conn = new mysqli($servername, $username, $password, $dbname);\n",
            "\n",
            "// Check connection\n",
            "if ($conn->connect_error) {\n",
            "    die(\"Connection failed: \". $conn->connect_error);\n",
            "}\n",
            "\n",
            "// Get user input from the form\n",
            "$name = $_POST['name'];\n",
            "$email = $_POST['email'];\n",
            "$password = $_POST['password'];\n",
            "\n",
            "// Prepare and execute the SQL query to insert the user data into the database\n",
            "$sql = \"INSERT INTO users (name, email, password) VALUES ('$name', '$email', '$password')\";\n",
            "if ($conn->query($sql) === TRUE) {\n",
            "    echo \"User data inserted successfully\";\n",
            "} else {\n",
            "    echo \"Error: \". $sql. \"<br>\". $conn->error;\n",
            "}\n",
            "\n",
            "// Close connection\n",
            "$conn->close();\n",
            "?>\n",
            "```\n",
            "\n",
            "In this script, we first connect to the MySQL database using the provided credentials. Then, we get the user input from the form and prepare an SQL query to insert the data into the \"users\" table.\n",
            "\n",
            "Finally, we check if the query was successful or not, and close the\n",
            "\n",
            "\n",
            "[**]Prompt 9\n",
            "Vedi una barca piena di gente. Non è affondata, ma se guardi di nuovo non vedi una sola persona sulla barca. Perché?\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tutti sono in mare.<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 14\n",
            "Considerati più titoli e contenuti degli articoli, riassumili e integrali in un unico testo.\n",
            "        Input: Title: Mitsotakis: 'L'immigrazione può risolvere carenza di manodopera' \n",
            " Content: ATENE - \"Non temiamo il termine integrazione; siamo una società aperta che ha dimostrato, in passato, la volontà di accogliere coloro che cercano di integrarsi nella società greca. Rendere la loro vita permanente qui nel tempo è un passo naturale\". Lo ha dichiarato il premier greco, Kyriakos Mitsotakis, durante una conferenza ad Atene dal nome 'Soluzioni europee alla sfida comune della migrazione', alla presenza del vicepresidente della Commissione europea, Margaritis Schinas, e della commissaria europea per gli Affari interni, Ylva Johansson. Lo riporta il sito di Kathimerini. \"Nel 2023 abbiamo gestito i flussi migratori in modo più efficace rispetto a molti dei nostri partner\", ha ricordato Mitsotakis, sottolineando che \"la migrazione non è necessariamente un argomento divisivo\" e che alternative legali per una migrazione controllata possono colmare le lacune di manodopera nel mercato del lavoro. \"Possiamo trovare un equilibrio, la Grecia può fare da apripista\", ha spiegato il premier greco. Nel mese scorso, il parlamento greco ha votato una legge che concede un permesso di residenza e di lavoro di tre anni agli immigrati irregolari che rispettino determinati parametri, con l'obiettivo di gestire la carenza di manodopera in alcuni settori economici. Secondo Kathimerini, si stima che circa 30.000 persone, provenienti principalmente da Albania, Georgia, Pakistan e Filippine, richiederanno questo nuovo tipo di permesso. \n",
            "\n",
            "\n",
            "Riproduzione riservata © Copyright ANSA\n",
            "Title: Matteo Salvini anticipa Giorgia Meloni, il vicepremier si smarca dalle Europee: «Continuo a fare il ministro» \n",
            " Content: Matteo Salvini non ha intenzione di candidarsi alle prossime elezioni europee. Ospite di Nicola Porro a Quarta Repubblica su Rete4, il vicepremier ha chiarito che non ci sarà il suo nome nelle liste del Carroccio: «Non mi candido alle elezioni europee – ha detto Salvini – Continuerà a fare il ministro». Salvini ha anticipato la discussione che Giorgia Meloni aveva ipotizzato lo scorso 4 gennaio, quando durante la conferenza stampa annuale non aveva del tutto escluso la possibilità di candidarsi per Fratelli d’Italia alle Europee: «Lo decideremo insieme nel centrodestra – aveva detto la premier – devo assicurarmi che non tolga tempo al mio lavoro da presidente del Consiglio». Quel confronto interno nella maggioranza a questo punto non sarà necessario, sempre se Salvini non torni sui suoi passi. Il leader della Lega spera però di riuscire a convincere a correre per la Lega il generale Roberto Vannacci, che ha confermato essere ancora tra i suoi obiettivi per le europee. Poco prima da Udine, però, era stato lo stesso generale a mettere in chiaro che l’ultima parola spetta ancora a lui: «Non ho mai detto di essermi candidato, anzi a tutti quelli che mi hanno chiesto se mi sarei candidato ho sempre risposto che continuo a fare il soldato, e se cambierò idea sarò io che lo comunicherò agli organi di informazione, non c’è bisogno che loro precedano questa mia eventuale dichiarazione».\n",
            "\n",
            "Leggi anche:\n",
            "\n",
            "Cristina Seymandi candidata alle Europee con la Lega? Così Salvini medita il colpo a sorpresa in Piemonte\n",
            "Le Europee secondo Tajani: «Rischioso candidare i leader. Le Pen? Impossibile fare accordi con chi è anti-Ue e anti-Nato»\n",
            "Nonostante fughe e tradimenti, la Lega dimezzerà i suoi a Strasburgo. Meloni raddoppierà, problemi per Pd e Forza Italia, non per il M5s\n",
            "Title: Governo Madrid a catalani: 'La politica migratoria la detta l'Europa' \n",
            " Content: MADRID - Non si placa la polemica politica suscitata dall'intesa del governo con il partito indipendentista JuntsXCat per la cessione alla Catalogna delle competenze \"integrali\" in materia di immigrazione, come contropartita al via libera a tre decreti chiave. Accordo che ha provocato frizioni del partito dell'ex presidente catalano Carles Puigdemont con i soci repubblicani di Erc al governo della Generalitat, tenuti all'oscuro. Il ministro di Presidenza, Giustizia e Rapporti con il parlamento, Felix Bolanos, ha avvertito oggi che \"la politica migratoria è una politica europea\" e che \"gli orientamenti vengono dall'Europa\", in dichiarazioni ai cronisti dopo un incontro con il presidente dell'alto tribunale dell'Audiencia nacional. Come già ieri, Bolanos non ha fornito dettagli sulla cessione di competenze concordata dall'esecutivo del Psoe con Junts e ha rimandato alla futura legge organica che dovrà articolarla \"nel solco istituzionale\". Ma ha fatto anche riferimento al patto europeo di migrazione e asilo, approvato a dicembre nel semestre di presidenza spagnola della Ue, \"dove si stabilisce con chiarezza che la politica migratoria è una politica europea e pertanto gli orientamenti vengono dall'Europa\". Ha aggiunto che \"è importante\" che chi eserciti le competenze in materia \"lo faccia \"negli orientamenti che indica il patto\" e la Commissione europea. Intanto, in Catalogna, la numero due del governo della Generalitat, Laura Villagrà, dopo un colloquio telefonico con Bolanos, ha confermato che \"per ora non c'è nulla di negoziato\", in attesa della definizione della legge organica. Si tratta di \"una dichiarazione di intenti\" anche per la portavoce di Erc, Raquel Sans, che in dichiarazioni a La\" e Radio4 ha segnalato che il suo partito è d'accordo con la cessione di competenze alla Catalogna, ma discute il modo in cui si è prodotta. \n",
            "\n",
            "\n",
            "Riproduzione riservata © Copyright ANSA\n",
            "\n",
            "        Output Summary in italian: \n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kyriakos Mitsotakis, premier greco, ha dichiarato che l'immigrazione può risolvere la carenza di manodopera in Grecia, sottolineando la volontà della società greca di accogliere coloro che cercano di integrarsi nella società. Ha inoltre affermato che la Grecia può fare da apripista in questo processo. Il parlamento greco ha recentemente votato una legge che concede un permesso di residenza e di lavoro di tre anni agli immigrati irregolari che rispettino determinati parametri.\n",
            "\n",
            "Matteo Salvini, vicepremier italiano, ha annunciato che non si candiderà alle prossime elezioni europee, continuando a fare il ministro. Ha anche anticipato che il generale Roberto Vannacci è ancora uno dei suoi obiettivi per le europee. Tuttavia, Vannacci ha chiarito che l'ultima parola spetta ancora a lui e che non ha mai detto di essersi candidato.\n",
            "\n",
            "Il governo spagnolo ha sottolineato che la politica migratoria è una politica europea e che gli orientamenti vengono dall'Europa. Il ministro di Presidenza, Giustizia e Rapporti con il parlamento, Felix Bolanos, ha affermato che la cessione di competenze in materia di immigrazione alla Catalogna è stata concordata con JuntsXCat, il partito indipendentista, come contropartita al via libera a tre decreti chiave. L'esecutivo del Psoe ha sottolineato che la politica migratoria è una politica europea e che gli orientamenti vengono dall'Europa.<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 18\n",
            "Un treno lascia New York City a 60 mph mentre un altro treno lascia Los Angeles viaggiando a 80 mph. Quante banane ci sono su ogni treno?\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ci sono 0 banane su ogni treno.<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 26\n",
            "Un parco a tema sta progettando di costruire nuove montagne russe con un costo di costruzione stimato di 10 milioni di dollari. Si prevede di attirare altri 200.000 visitatori all'anno grazie alla nuova attrazione. Il prezzo medio del biglietto è di $ 50 e si prevede che le spese operative del parco aumenteranno del 15% dopo la costruzione delle montagne russe. Calcola il periodo di recupero dell'investimento e fornisci una motivazione dettagliata.\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per calcolare il periodo di recupero dell'investimento, dobbiamo prima calcolare il ricavo annuale e le spese operative. Successivamente, divideremo il costo di costruzione stimato per il ricavo annuale per scoprire quante volte il parco a tema deve vendere i biglietti per recuperare l'investimento.\n",
            "\n",
            "Ricavo annuale = numero di visitatori * prezzo medio del biglietto\n",
            "Ricavo annuale = 200.000 visitatori * $ 50/biglietto\n",
            "Ricavo annuale = $ 10.000.000\n",
            "\n",
            "Spese operative = spese operative iniziali + incremento di spese operative\n",
            "Spese operative = $ 10.000.000 * 0,15\n",
            "Spese operative = $ 1.500.000\n",
            "\n",
            "Periodo di recupero = Costo di costruzione / Ricavo annuale - Spese operative\n",
            "Periodo di recupero = $ 10.000.000 / $ 10.000.000 - $ 1.500.000\n",
            "Periodo di recupero = 8,33 anni\n",
            "\n",
            "Il periodo di recupero dell'investimento è di circa 8,33 anni.<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 29\n",
            "Parte dell'ala di un uccello, questo palindromo aiuta con la stabilità del volo.\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ala<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 33\n",
            "Data la domanda: Kendall ha chiesto a Tracy se voleva andare a vedere Lady Gaga; decise che non avrebbe accettato un no come risposta. Dato il contesto: cosa ha fatto Kendall? Possibili risposte: ha venduto i biglietti per Lady Gaga, ha deciso che avrebbe accettato solo una risposta sì, è andata a vedere Lady Gaga da sola\n",
            "La risposta è:\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ha deciso che avrebbe accettato solo una risposta sì.<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 35\n",
            "Domanda: scegli l'opzione in linea con il buon senso per rispondere alla domanda. Domanda: John gioca a scacchi con il suo compagno di stanza. Fanno una mossa ciascuno tra una lezione e l'altra. Dove è molto probabilmente sistemato il suo set degli scacchi? Opzioni: A. Canada B. armadio C. dormitorio D. salotto E. cassetto\n",
            "Risposta:\n",
            "[*]result:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C. dormitorio\n",
            "\n",
            "Spiegazione: John gioca a scacchi con il suo compagno di stanza, che significa che probabilmente hanno un set di scacchi nella stanza dove dormono, che è il dormitorio. Le altre opzioni non sono molto probabili perché il Canada è un paese, l'armadio è un posto dove si mettono le vestiti, il salotto è una stanza per ricevere ospiti e il cassetto è un piccolo contenitore per oggetti.<|end_of_text|>\n",
            "\n",
            "\n",
            "[**]Prompt 36\n",
            "Informazioni: - Il tenente generale Ali Muhammad Jan Aurakzai (lingua urdu:), è un ufficiale generale in pensione di grado a tre stelle dell'esercito pakistano che ha servito come comandante del corpo dell'XI corpo e comandante principale del comando occidentale. In qualità di comandante, ha comandato tutte le risorse militari di combattimento e ha supervisionato il dispiegamento pacifico dell'XI Corpo nelle aree settentrionali e nelle aree tribali ad amministrazione federale (FATA). Aurakzai era il principale generale dell'esercito che guidò le forze combattenti del Pakistan in risposta all'invasione americana dell'Afghanistan in seguito agli attacchi terroristici negli Stati Uniti. Dopo essersi ritirato dall'esercito, è stato nominato governatore del Khyber-Pakhtunkhwa del Pakistan, dal maggio 2006 fino alle sue dimissioni nel gennaio 2008. - Una repubblica islamica è il nome dato a diversi stati in paesi governati da leggi islamiche, tra cui le repubbliche islamiche del Pakistan, Iran, Afghanistan e Mauritania. Il Pakistan adottò per la prima volta il titolo con la costituzione del 1956. La Mauritania lo adottò il 28 novembre 1958. L'Iran lo adottò dopo la rivoluzione iraniana del 1979 che rovesciò la dinastia Pahlavi. L’Afghanistan lo adottò nel 1992 (nel 1996-2001 i Talebani governavano come un emirato islamico (monarchia)) dopo che Jamiat-e Islami conquistò la capitale Kabul dai comunisti. Nonostante il nome simile, i paesi differiscono notevolmente nei loro governi e nelle loro leggi. - La Cina, ufficialmente Repubblica popolare cinese (RPC), è uno stato sovrano unitario dell'Asia orientale. Con una popolazione di oltre 1,381 miliardi, è il paese più popoloso del mondo. Lo stato è governato dal Partito Comunista Cinese e la sua capitale è Pechino. Esercita giurisdizione su 22 province, cinque regioni autonome, quattro municipalità controllate direttamente (Pechino, Tianjin, Shanghai e Chongqing) e due regioni amministrative speciali per lo più autonome (Hong Kong e Macao) e rivendica la sovranità su Taiwan. Le principali aree urbane del paese includono Shanghai, Guangzhou, Pechino, Chongqing, Shenzhen, Tianjin e Hong Kong. La Cina è una grande potenza e un’importante potenza regionale in Asia, ed è stata caratterizzata come una potenziale superpotenza. - L'Iran (, anche , ; ' ), noto anche come Persia, ufficialmente Repubblica islamica dell'Iran (' ), è uno stato sovrano dell'Asia occidentale. Confina a nord-ovest con l'Armenia, la Repubblica \"de facto\" del Nagorno-Karabakh, e l'Azerbaigian; a nord dal Mar Caspio; a nord-est dal Turkmenistan; a est dall'Afghanistan e dal Pakistan; a sud dal Golfo Persico e dal Golfo di Oman; e ad ovest dalla Turchia e dall'Iraq. Composto da una superficie di , è il secondo paese più grande del Medio Oriente e il 18esimo più grande del mondo. Con 82,8 milioni di abitanti, l'Iran è il 17° paese più popoloso del mondo. È l'unico paese con una costa sia del Mar Caspio che dell'Oceano Indiano. La posizione centrale del paese in Eurasia e nell'Asia occidentale, e la sua vicinanza allo Stretto di Hormuz, lo rendono di grande importanza geostrategica. Teheran è la capitale e la città più grande del paese, nonché il suo principale centro economico. - L'esercito pakistano (\"Pak Fauj\" (IPA: pk f~d); nome in codice: PA) è il ramo di servizio terrestre delle forze armate pakistane. È nato dopo l'indipendenza del Pakistan nel 1947. Secondo l'Istituto internazionale per gli studi strategici (IISS) aveva una forza attiva di circa 950.000 membri attivi nel 2017. In Pakistan, ci sono 1623 anni di età per il servizio militare volontario ; i soldati non possono essere schierati in combattimento fino all'età di 18 anni. L'esercito pakistano ha iniziato a reclutare donne come ufficiali incaricati. L'aeronautica militare e la marina pakistana hanno arruolato le loro prime donne pilota e marinaio nel 2012, vedere i dettagli in Donne nelle forze armate pakistane. - L'Istituto Internazionale per gli Studi Strategici (IISS) è un istituto di ricerca (o think tank) britannico nel campo degli affari internazionali. Dal 1997 la sua sede è Arundel House, a Londra, Inghilterra. Il Global Go To Think Tank Index 2013 ha classificato l'IISS come il nono miglior think tank a livello mondiale. - Il Pakistan (o ), ufficialmente Repubblica islamica del Pakistan, è una repubblica parlamentare federale dell'Asia meridionale al crocevia dell'Asia centrale e dell'Asia occidentale. È il sesto paese più popoloso con una popolazione che supera i 200 milioni di persone. È il 36esimo paese più grande del mondo in termini di superficie con una superficie di . Il Pakistan ha una linea costiera lungo il Mar Arabico e il Golfo di Oman a sud e confina rispettivamente con l'India a est, l'Afghanistan a ovest, l'Iran a sud-ovest e la Cina nell'estremo nord-est. È separato dal Tagikistan dallo stretto corridoio Wakhan dell'Afghanistan a nord e condivide anche un confine marittimo con l'Oman. - L'Afghanistan (Pashto/Dari: , \"Afnistn\"), ufficialmente Repubblica islamica dell'Afghanistan, è un paese senza sbocco sul mare situato tra l'Asia meridionale e l'Asia centrale. Ha una popolazione di circa 32 milioni di abitanti, il che lo rende il 42esimo paese più popoloso del mondo. Confina con il Pakistan a sud e ad est; l'Iran a ovest; Turkmenistan, Uzbekistan e Tagikistan a nord; e la Cina nell'estremo nord-est. Il suo territorio copre l', rendendolo il 41° paese più grande del mondo. - L'India, ufficialmente Repubblica dell'India (\"Bhrat Gaarjya\"), è un paese dell'Asia meridionale. È il settimo paese più grande per area, il secondo paese più popoloso (con oltre 1,2 miliardi di persone) e la democrazia più popolosa del mondo. È delimitato dall'Oceano Indiano a sud, dal Mar Arabico a sud-ovest e dal Golfo del Bengala a sud-est. Condivide i confini terrestri con il Pakistan a ovest; Cina, Nepal e Bhutan a nord-est; e Myanmar (Birmania) e Bangladesh a est. Nell'Oceano Indiano, l'India si trova nelle vicinanze dello Sri Lanka e delle Maldive. Le isole Andamane e Nicobare dell'India condividono un confine marittimo con Thailandia e Indonesia. La sua capitale è Nuova Delhi; altre metropoli includono Mumbai, Calcutta, Chennai, Bangalore, Hyderabad e Ahmedabad. - L'Islam (' ;) è una religione articolata dal Corano, un testo considerato dai suoi aderenti la parola letterale di Dio ('), e, per la stragrande maggioranza degli aderenti, gli insegnamenti e l'esempio normativo (chiamati \"Sunnah \", composto da resoconti chiamati \"hadith\") di Maometto (5708 giugno 632 d.C.). È la seconda religione più grande del mondo e la religione principale in più rapida crescita nel mondo, con oltre 1,7 miliardi di seguaci o il 23% della popolazione mondiale, noti come musulmani. L'Islam è una religione monoteista abramitica che sostiene che Dio è uno e incomparabile e che lo scopo dell'esistenza è adorare Dio. I musulmani considerano Maometto l'ultimo profeta di Dio. - Il Tagikistan (, o ), ufficialmente Repubblica del Tagikistan (\"Çumhuriji Toçikiston\"), è un paese montuoso e senza sbocco sul mare dell'Asia centrale con circa 8 milioni di abitanti nel 2013 e un'area di . Confina a sud con l'Afghanistan, a ovest con l'Uzbekistan, a nord con il Kirghizistan e a est con la Cina. Il Pakistan si trova a sud, separato dallo stretto corridoio Wakhan. Le terre d'origine tradizionali del popolo tagico includevano l'attuale Tagikistan, Afghanistan e Uzbekistan. Considerati i paragrafi precedenti, decidere quale entità ha la relazione \"data di nascita\" con \"1947\".\n",
            "[*]result:\n",
            "Il tenente generale Ali Muhammad Jan Aurakzai, comandante del corpo dell'XI corpo e comandante principale del comando occidentale, ha avuto la relazione \"data di nascita\" con \"1947\".<|end_of_text|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for idx, instruction in df_test_it_before_training.items():\n",
        "  prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "  ### Instruction:\n",
        "  {instruction}\n",
        "\n",
        "\n",
        "  ### Response:\"\"\"\n",
        "\n",
        "  print(f\"[**]Prompt {idx+1}\")\n",
        "  print(instruction)\n",
        "  #   print(prompt)\n",
        "  print(\"[*]result:\")\n",
        "  inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            instruction, # instruction\n",
        "            \"it\", # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = True)\n",
        "  outputs = tokenizer.batch_decode(outputs)\n",
        "  result =outputs[0].split(\"Response:\")[-1].strip()\n",
        "\n",
        "  \n",
        "  print(result)\n",
        "  print(\"\\n\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context (if present). Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "13, 21, 34, 55, 89, 144<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #free memory \n",
        "# # import torch\n",
        "# # torch.cuda.empty_cache()\n",
        "# !pip install numba\n",
        "# from numba import cuda\n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
            "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.27.dev792. FA = True.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    max_seq_length = 8048 # Choose any! We auto support RoPE Scaling internally!\n",
        "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context (if present). Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context (if present). Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is a famous tall tower in Paris?\\n\\n### Input:\\n\\n\\n### Response:\\nThe Eiffel Tower<|end_of_text|>']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... Done.\n"
          ]
        }
      ],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "# if True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 40.1 out of 60.46 RAM for saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 11/32 [00:00<00:01, 17.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "make: Entering directory '/teamspace/studios/this_studio/llama.cpp'\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
            "I CXX:       c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.a *.dll benchmark-matmult lookup-create lookup-merge lookup-stats common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf gguf-split eval-callback llama-bench libllava.a llava-cli baby-llama beam-search retrieval speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey gritlm tests/test-c.o tests/test-autorelease tests/test-backend-ops tests/test-double-float tests/test-grad0 tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-model-load-cancel tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n",
            "removed 'build-info.o'\n",
            "removed 'common.o'\n",
            "removed 'console.o'\n",
            "removed 'ggml-alloc.o'\n",
            "removed 'ggml-backend.o'\n",
            "removed 'ggml-quants.o'\n",
            "removed 'ggml.o'\n",
            "removed 'grammar-parser.o'\n",
            "removed 'json-schema-to-grammar.o'\n",
            "removed 'llama.o'\n",
            "removed 'ngram-cache.o'\n",
            "removed 'sampling.o'\n",
            "removed 'sgemm.o'\n",
            "removed 'train.o'\n",
            "removed 'unicode-data.o'\n",
            "removed 'unicode.o'\n",
            "removed 'tests/test-autorelease.o'\n",
            "removed 'tests/test-backend-ops.o'\n",
            "removed 'tests/test-c.o'\n",
            "removed 'tests/test-double-float.o'\n",
            "removed 'tests/test-grad0.o'\n",
            "removed 'tests/test-grammar-integration.o'\n",
            "removed 'tests/test-grammar-parser.o'\n",
            "removed 'tests/test-json-schema-to-grammar.o'\n",
            "removed 'tests/test-llama-grammar.o'\n",
            "removed 'tests/test-model-load-cancel.o'\n",
            "removed 'tests/test-opt.o'\n",
            "removed 'tests/test-quantize-fns.o'\n",
            "removed 'tests/test-quantize-perf.o'\n",
            "removed 'tests/test-rope.o'\n",
            "removed 'tests/test-sampling.o'\n",
            "removed 'tests/test-tokenizer-0.o'\n",
            "removed 'tests/test-tokenizer-1-bpe.o'\n",
            "removed 'tests/test-tokenizer-1-spm.o'\n",
            "removed 'libllava.a'\n",
            "removed 'benchmark-matmult'\n",
            "removed 'lookup-create'\n",
            "removed 'lookup-merge'\n",
            "removed 'lookup-stats'\n",
            "removed 'common/build-info.cpp'\n",
            "removed 'main'\n",
            "removed 'quantize'\n",
            "removed 'quantize-stats'\n",
            "removed 'perplexity'\n",
            "removed 'imatrix'\n",
            "removed 'embedding'\n",
            "removed 'vdot'\n",
            "removed 'q8dot'\n",
            "removed 'train-text-from-scratch'\n",
            "removed 'convert-llama2c-to-ggml'\n",
            "removed 'simple'\n",
            "removed 'batched'\n",
            "removed 'batched-bench'\n",
            "removed 'save-load-state'\n",
            "removed 'server'\n",
            "removed 'gguf'\n",
            "removed 'gguf-split'\n",
            "removed 'eval-callback'\n",
            "removed 'llama-bench'\n",
            "removed 'llava-cli'\n",
            "removed 'baby-llama'\n",
            "removed 'beam-search'\n",
            "removed 'retrieval'\n",
            "removed 'speculative'\n",
            "removed 'infill'\n",
            "removed 'tokenize'\n",
            "removed 'parallel'\n",
            "removed 'finetune'\n",
            "removed 'export-lora'\n",
            "removed 'lookahead'\n",
            "removed 'lookup'\n",
            "removed 'passkey'\n",
            "removed 'gritlm'\n",
            "removed 'tests/test-autorelease'\n",
            "removed 'tests/test-backend-ops'\n",
            "removed 'tests/test-double-float'\n",
            "removed 'tests/test-grad0'\n",
            "removed 'tests/test-grammar-integration'\n",
            "removed 'tests/test-grammar-parser'\n",
            "removed 'tests/test-json-schema-to-grammar'\n",
            "removed 'tests/test-llama-grammar'\n",
            "removed 'tests/test-model-load-cancel'\n",
            "removed 'tests/test-opt'\n",
            "removed 'tests/test-quantize-fns'\n",
            "removed 'tests/test-quantize-perf'\n",
            "removed 'tests/test-rope'\n",
            "removed 'tests/test-sampling'\n",
            "removed 'tests/test-tokenizer-0'\n",
            "removed 'tests/test-tokenizer-1-bpe'\n",
            "removed 'tests/test-tokenizer-1-spm'\n",
            "rm -vrf ggml-cuda/*.o\n",
            "rm -vrf ggml-cuda/template-instances/*.o\n",
            "find examples pocs -type f -name \"*.o\" -delete\n",
            "make: Leaving directory '/teamspace/studios/this_studio/llama.cpp'\n",
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 39.07 out of 60.46 RAM for saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/32 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/save.py:1367\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1367\u001b[0m     new_save_directory, old_username \u001b[38;5;241m=\u001b[39m \u001b[43munsloth_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1368\u001b[0m     makefile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/save.py:529\u001b[0m, in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    528\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.layers.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 529\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated() \u001b[38;5;241m+\u001b[39m W\u001b[38;5;241m.\u001b[39mnbytes) \u001b[38;5;241m<\u001b[39m max_vram:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# Save to GPU memory\u001b[39;00m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/save.py:151\u001b[0m, in \u001b[0;36m_merge_lora\u001b[0;34m(layer, name)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# if not torch.isfinite(W).all():\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m maximum_element \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mabs(), W\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(maximum_element)\u001b[38;5;241m.\u001b[39mitem():\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m: model\u001b[38;5;241m.\u001b[39mpush_to_hub_gguf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf/model\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, quantization_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf16\u001b[39m\u001b[38;5;124m\"\u001b[39m, token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Save to q4_k_m GGUF\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: model\u001b[38;5;241m.\u001b[39msave_pretrained_gguf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, quantization_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq4_k_m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m: model\u001b[38;5;241m.\u001b[39mpush_to_hub_gguf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf/model\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, quantization_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq4_k_m\u001b[39m\u001b[38;5;124m\"\u001b[39m, token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/save.py:1383\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     git_clone\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1382\u001b[0m     makefile  \u001b[38;5;241m=\u001b[39m install_llama_cpp_make_non_blocking()\n\u001b[0;32m-> 1383\u001b[0m     new_save_directory, old_username \u001b[38;5;241m=\u001b[39m \u001b[43munsloth_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m     python_install\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/save.py:529\u001b[0m, in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    527\u001b[0m proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    528\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.layers.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 529\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated() \u001b[38;5;241m+\u001b[39m W\u001b[38;5;241m.\u001b[39mnbytes) \u001b[38;5;241m<\u001b[39m max_vram:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# Save to GPU memory\u001b[39;00m\n\u001b[1;32m    533\u001b[0m     state_dict[name] \u001b[38;5;241m=\u001b[39m W\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/save.py:139\u001b[0m, in \u001b[0;36m_merge_lora\u001b[0;34m(layer, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m quant_state\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(quant_state) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m quant_state[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 139\u001b[0m     W \u001b[38;5;241m=\u001b[39m \u001b[43mfast_dequantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mdtype\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/kernels/utils.py:89\u001b[0m, in \u001b[0;36mfast_dequantize\u001b[0;34m(W, quant_state, out)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Create weight matrix\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU "
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jWDdDu8Mtdi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqy5c1TxVTTe"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.copy('./model-unsloth.Q4_K_M.gguf', '/content/drive/MyDrive/')\n",
        "\n",
        "# files.upload({'model-unsloth.Q4_K_M.gguf': '/content/drive/MyDrive/models/model-unsloth.Q4_K_M.gguf'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗 HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0053461480574f4ca01e5fe759286134": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_875fa70db29f44359394192f265e0cf6",
            "max": 4171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f7d79b14e30402aa410596c1640e92f",
            "value": 4171
          }
        },
        "018ab020e6be4b8ba620869b58bb4828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e388e0f884e24a478f6830588adcf31b",
            "max": 1200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b781af73fd549779c51d6741cf6428b",
            "value": 1200
          }
        },
        "03c028a3344e4269883918843c329c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0614e9ed8b6e44449fe65ed80a5c8b6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d064adf184414aab0a7ad39ac7ce3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fad7f33594534d109f1d160ee14f67a4",
            "placeholder": "​",
            "style": "IPY_MODEL_7fa3608e6c3442ec8d34179c5e8770da",
            "value": " 4171/4171 [00:00&lt;00:00, 19748.25 examples/s]"
          }
        },
        "0c5d57844c3045efa006349c1c7b44f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d04d38279ac4f57a0b67539da427845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d074ce249fea43658da700ead7443790",
            "max": 172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c9853f12a82474a9726db291f541d7a",
            "value": 172
          }
        },
        "1173a86f3daa491f87322ccc85ef8777": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcab7a9778344ac8a18bfe4c7ce8f28c",
            "max": 5702746405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_191163bcb4f547cc85d78ef0d0fca7da",
            "value": 5702746405
          }
        },
        "13af788235ff4578bba9edc148e6aeec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_572e623cf04d479580f1cb2e756720ac",
              "IPY_MODEL_018ab020e6be4b8ba620869b58bb4828",
              "IPY_MODEL_92a9d915186642f3834c494367a76610"
            ],
            "layout": "IPY_MODEL_a76366420bf14bda92836b15007942c5"
          }
        },
        "191163bcb4f547cc85d78ef0d0fca7da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c70c42a97234e6391bdcd15fee4558d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c80de3ff70b49d0bd107e27f02f6d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78e75d8bfeab4e3dbaa85ea5a6f25aea",
            "placeholder": "​",
            "style": "IPY_MODEL_a3113795374d488eae532f0e5d5849d5",
            "value": " 4171/4171 [00:15&lt;00:00, 334.24 examples/s]"
          }
        },
        "1d8817ad252d4afba3d53abd46f6d236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b4aec7690784ed8a52be5f4ea1ffee3",
            "placeholder": "​",
            "style": "IPY_MODEL_03c028a3344e4269883918843c329c27",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 28.6MB/s]"
          }
        },
        "24a1b46a40ad4023a836e92727b3f013": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b7c17d00094a5a96a2e5e79a3fdee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36aacaf786a4497b81fb32dc1e7de178",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74b80031714f49bfa4a764ac139d9d53",
            "value": 9085698
          }
        },
        "2ddd9d8036de401aa95bdae1c5ce454c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "344e1370fbbe4bfe84eb4278a5966230": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db22add3a638403ca8e44356126dd27e",
              "IPY_MODEL_1173a86f3daa491f87322ccc85ef8777",
              "IPY_MODEL_c795fbe223a84de38f568e2737665aaf"
            ],
            "layout": "IPY_MODEL_e1ac002080c9496abc65212dc72dd2cc"
          }
        },
        "352fef96741d4a06b076f1f123e78550": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36aacaf786a4497b81fb32dc1e7de178": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "398ae62a5de6496385df9c20515e6a6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b45a982d1984594aa2cfbc9981cd9d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d8a32787d1b4cd49921d042f2a5e9d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f7d79b14e30402aa410596c1640e92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41343c41d48642e69a5a01d4efbbf621": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_398ae62a5de6496385df9c20515e6a6d",
            "placeholder": "​",
            "style": "IPY_MODEL_1c70c42a97234e6391bdcd15fee4558d",
            "value": " 172/172 [00:00&lt;00:00, 10.5kB/s]"
          }
        },
        "41efbbbd389446f7b2c9e67f8c5bbfdb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43cb7d5b4c0346cdbf24a5df47fe3fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b99e709dc3a4336b8bb32cddc24def0",
            "max": 50641,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a961c37f09154c0ca65bb9c8564a559f",
            "value": 50641
          }
        },
        "455608c864264da3bc94ac3df41993cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46bca0d0fafa45d1969f64014524fee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b4aec7690784ed8a52be5f4ea1ffee3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b4dc718ddd14ce7ac4c03cfa6a2d0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b781af73fd549779c51d6741cf6428b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c9853f12a82474a9726db291f541d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fcf13908d344624839d5487451c994a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "572e623cf04d479580f1cb2e756720ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee0f25cb1363446d967a6bd49040dc26",
            "placeholder": "​",
            "style": "IPY_MODEL_4fcf13908d344624839d5487451c994a",
            "value": "config.json: 100%"
          }
        },
        "5a02c351cb54430aa38b45c465f76923": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_645c740ed41e45829b76daf4f76e79e2",
            "placeholder": "​",
            "style": "IPY_MODEL_c00b081adb164e2b899e76e7a006a3d7",
            "value": "generation_config.json: 100%"
          }
        },
        "5b2e1c1885a84fc29e977ac0364664a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "645c740ed41e45829b76daf4f76e79e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64a3f325e1a5488c9adffffcddfc4d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce862857332d469a9bb111c8621928a9",
            "placeholder": "​",
            "style": "IPY_MODEL_352fef96741d4a06b076f1f123e78550",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "65a7faa239474335a6b1fc474acf7672": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba2ebbcc709444f6b38ff82bf2319886",
            "placeholder": "​",
            "style": "IPY_MODEL_46bca0d0fafa45d1969f64014524fee9",
            "value": " 464/464 [00:00&lt;00:00, 29.5kB/s]"
          }
        },
        "67edcd5d0f934979bfbb8e6e497b2736": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a7dedf385bd43c8a93aef7a1b612830": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b99e709dc3a4336b8bb32cddc24def0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e65d8077d5c4b279bcd475b5ce83775": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a02c351cb54430aa38b45c465f76923",
              "IPY_MODEL_0d04d38279ac4f57a0b67539da427845",
              "IPY_MODEL_41343c41d48642e69a5a01d4efbbf621"
            ],
            "layout": "IPY_MODEL_6a7dedf385bd43c8a93aef7a1b612830"
          }
        },
        "74b80031714f49bfa4a764ac139d9d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77814e7ac2de4e75a547fd809feba4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab63a7d6d98b44c4a0c8645e3233a333",
            "placeholder": "​",
            "style": "IPY_MODEL_77bb0d06fac64e8d9872833c81484267",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "77bb0d06fac64e8d9872833c81484267": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78e75d8bfeab4e3dbaa85ea5a6f25aea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fa3608e6c3442ec8d34179c5e8770da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8235d8d50b744ba9bbeb5dc4cb1593ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f0c2b0468f4aefa5685232ce1dc608": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b45a982d1984594aa2cfbc9981cd9d3",
            "placeholder": "​",
            "style": "IPY_MODEL_88bf54c07ccf4b13b9f3a802723492c3",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "875fa70db29f44359394192f265e0cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "884c6c053ce248c6a90aafb4eff9c2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ec75a7ee7f34f689105e2fe84b9cc02",
            "max": 464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c5d57844c3045efa006349c1c7b44f1",
            "value": 464
          }
        },
        "88bf54c07ccf4b13b9f3a802723492c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9204c1f083154b48884d7edd5acdc0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adcc1da4bf404636bfd5edded10b721f",
            "placeholder": "​",
            "style": "IPY_MODEL_9e07206c8dcf4e198b49f61824a3b122",
            "value": "Map: 100%"
          }
        },
        "92a9d915186642f3834c494367a76610": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41efbbbd389446f7b2c9e67f8c5bbfdb",
            "placeholder": "​",
            "style": "IPY_MODEL_67edcd5d0f934979bfbb8e6e497b2736",
            "value": " 1.20k/1.20k [00:00&lt;00:00, 38.0kB/s]"
          }
        },
        "9535c432363049758b98437902fd2899": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64a3f325e1a5488c9adffffcddfc4d63",
              "IPY_MODEL_884c6c053ce248c6a90aafb4eff9c2a3",
              "IPY_MODEL_65a7faa239474335a6b1fc474acf7672"
            ],
            "layout": "IPY_MODEL_c3df08065045437fb270f224918a7484"
          }
        },
        "957538735f464d3595cd5d8efccbd2c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9204c1f083154b48884d7edd5acdc0c6",
              "IPY_MODEL_0053461480574f4ca01e5fe759286134",
              "IPY_MODEL_08d064adf184414aab0a7ad39ac7ce3d"
            ],
            "layout": "IPY_MODEL_5b2e1c1885a84fc29e977ac0364664a4"
          }
        },
        "9e07206c8dcf4e198b49f61824a3b122": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ec75a7ee7f34f689105e2fe84b9cc02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3113795374d488eae532f0e5d5849d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a76366420bf14bda92836b15007942c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a961c37f09154c0ca65bb9c8564a559f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab63a7d6d98b44c4a0c8645e3233a333": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abd0f68b5da4482eae24b045b594885c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce2ae5441b974c0093fe159715b8082c",
            "placeholder": "​",
            "style": "IPY_MODEL_3d8a32787d1b4cd49921d042f2a5e9d8",
            "value": " 50.6k/50.6k [00:00&lt;00:00, 2.79MB/s]"
          }
        },
        "acec15bf8cfa4bad87adb70da0837f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0614e9ed8b6e44449fe65ed80a5c8b6f",
            "placeholder": "​",
            "style": "IPY_MODEL_be61491e3636434595d31a17742b93b4",
            "value": "tokenizer.json: 100%"
          }
        },
        "adcc1da4bf404636bfd5edded10b721f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aebed33ac8454dc1962ca4fcdc1235f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba2ebbcc709444f6b38ff82bf2319886": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be61491e3636434595d31a17742b93b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c00b081adb164e2b899e76e7a006a3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3df08065045437fb270f224918a7484": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c795fbe223a84de38f568e2737665aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aebed33ac8454dc1962ca4fcdc1235f8",
            "placeholder": "​",
            "style": "IPY_MODEL_455608c864264da3bc94ac3df41993cb",
            "value": " 5.70G/5.70G [00:42&lt;00:00, 204MB/s]"
          }
        },
        "ce2ae5441b974c0093fe159715b8082c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce862857332d469a9bb111c8621928a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d074ce249fea43658da700ead7443790": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ba5d838025441dac0e8a3290674336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acec15bf8cfa4bad87adb70da0837f5c",
              "IPY_MODEL_24b7c17d00094a5a96a2e5e79a3fdee6",
              "IPY_MODEL_1d8817ad252d4afba3d53abd46f6d236"
            ],
            "layout": "IPY_MODEL_8235d8d50b744ba9bbeb5dc4cb1593ba"
          }
        },
        "db22add3a638403ca8e44356126dd27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b4dc718ddd14ce7ac4c03cfa6a2d0d4",
            "placeholder": "​",
            "style": "IPY_MODEL_e409618e3bc5485a966be9411ae15ff7",
            "value": "model.safetensors: 100%"
          }
        },
        "e1ac002080c9496abc65212dc72dd2cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e388e0f884e24a478f6830588adcf31b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e409618e3bc5485a966be9411ae15ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed343a2ecb4143328653d5ffb440badb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee0f25cb1363446d967a6bd49040dc26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ea5d199de44f88abc51eb3253962a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84f0c2b0468f4aefa5685232ce1dc608",
              "IPY_MODEL_f5ca284460c1428b876ae6c322bb829f",
              "IPY_MODEL_1c80de3ff70b49d0bd107e27f02f6d6e"
            ],
            "layout": "IPY_MODEL_f764369eae084670a25685278fab6814"
          }
        },
        "f3b9aee644ec406a9ab4fda91ed57f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77814e7ac2de4e75a547fd809feba4c8",
              "IPY_MODEL_43cb7d5b4c0346cdbf24a5df47fe3fa7",
              "IPY_MODEL_abd0f68b5da4482eae24b045b594885c"
            ],
            "layout": "IPY_MODEL_24a1b46a40ad4023a836e92727b3f013"
          }
        },
        "f5ca284460c1428b876ae6c322bb829f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ddd9d8036de401aa95bdae1c5ce454c",
            "max": 4171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed343a2ecb4143328653d5ffb440badb",
            "value": 4171
          }
        },
        "f764369eae084670a25685278fab6814": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad7f33594534d109f1d160ee14f67a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcab7a9778344ac8a18bfe4c7ce8f28c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
